{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SPE DSEATS AFRICA - DATATHON 2025\n",
        "**Overview**\n",
        "\n",
        "The Society of Petroleum Engineer (SPE) Data Science and Engineering Analytics Technical Section (DSEATS) Africa Datathon Challenge presents an exciting opportunity for participants to harness the power of machine learning to address real-world challenges in the oil and gas industry. Organized by SPE DSEATS Africa Region, this challenge aims to leverage historical production data to accurately classify a group of 20 wells based on their observed performance trends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Packages Required**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Data Phandas SPE DSEATS Africa Datathon 2025 - Well Classification Pipeline with XGBoost\n",
        "Complete pipeline from data cleaning to XGBoost classification with hyperparameter tuning\n",
        "Now includes Formation Volume Factor (FVF) and reservoir barrel calculations\n",
        "\"\"\"\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOAD THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WellClassificationPipeline:\n",
        "    \"\"\"\n",
        "    Complete pipeline for well classification using XGBoost with hyperparameter tuning\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.wells_data = None\n",
        "        self.reservoir_info = None\n",
        "        self.classification_params = None\n",
        "        self.processed_features = None\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "        self.models = {}\n",
        "        self.best_params = {}\n",
        "        self.target_columns = [\n",
        "            'Reservoir Name', 'Reservoir Type', 'Well Type', 'Production Type',\n",
        "            'Formation GOR Trend', 'Watercut Trend', 'Oil Productivity Index Trend'\n",
        "        ]\n",
        "        \n",
        "        # Initialize reservoir FVF mapping (will be populated in load_data)\n",
        "        self.reservoir_fvf = None\n",
        "        \n",
        "    def load_data(self):\n",
        "        \"\"\"Load and examine the datasets\"\"\"\n",
        "        print(\"Loading datasets...\")\n",
        "        \n",
        "        # Load wells production data\n",
        "        self.wells_data = pd.read_csv('data/spe_africa_dseats_datathon_2025_wells_dataset.csv')\n",
        "        \n",
        "        # Load reservoir information\n",
        "        self.reservoir_info = pd.read_csv('data/reservoir_info.csv')\n",
        "        \n",
        "        # Create FVF mapping (RB/STB) from reservoir info\n",
        "        self.reservoir_fvf = dict(zip(\n",
        "            self.reservoir_info['Reservoir Name'],\n",
        "            self.reservoir_info['Formation Volume Factor (RB/STB)']\n",
        "        ))\n",
        "        \n",
        "        # Load classification parameters (target classes)\n",
        "        self.classification_params = pd.read_csv('data/classification_parameters.csv')\n",
        "        \n",
        "        print(f\"Wells data shape: {self.wells_data.shape}\")\n",
        "        print(f\"Reservoir info shape: {self.reservoir_info.shape}\")\n",
        "        print(f\"Classification params shape: {self.classification_params.shape}\")\n",
        "        \n",
        "        # Display basic info\n",
        "        print(\"\\nWells data columns:\")\n",
        "        print(self.wells_data.columns.tolist())\n",
        "        print(f\"\\nUnique wells: {self.wells_data['WELL_NAME'].nunique()}\")\n",
        "        print(f\"Date range: {self.wells_data['PROD_DATE'].min()} to {self.wells_data['PROD_DATE'].max()}\")\n",
        "        print(\"\\nReservoir FVF values:\")\n",
        "        print(self.reservoir_fvf)\n",
        "        \n",
        "    def clean_data(self):\n",
        "        \"\"\"Clean and preprocess the raw data\"\"\"\n",
        "        print(\"\\nCleaning data...\")\n",
        "        \n",
        "        # Convert date column\n",
        "        self.wells_data['PROD_DATE'] = pd.to_datetime(self.wells_data['PROD_DATE'])\n",
        "        \n",
        "        # Clean numeric columns that might have commas\n",
        "        numeric_columns = [\n",
        "            'BOTTOMHOLE_FLOWING_PRESSURE (PSI)',\n",
        "            'ANNULUS_PRESS (PSI)', \n",
        "            'WELL_HEAD_PRESSURE (PSI)',\n",
        "            'CUMULATIVE_OIL_PROD (STB)',\n",
        "            'CUMULATIVE_FORMATION_GAS_PROD (MSCF)',\n",
        "            'CUMULATIVE_TOTAL_GAS_PROD (MSCF)',\n",
        "            'CUMULATIVE_WATER_PROD (BBL)'\n",
        "        ]\n",
        "        \n",
        "        for col in numeric_columns:\n",
        "            if col in self.wells_data.columns:\n",
        "                # Remove commas and convert to numeric\n",
        "                self.wells_data[col] = pd.to_numeric(\n",
        "                    self.wells_data[col].astype(str).str.replace(',', ''),\n",
        "                    errors='coerce'\n",
        "                )\n",
        "        \n",
        "        # Handle missing values\n",
        "        self.wells_data = self.wells_data.fillna(0)\n",
        "        \n",
        "        # Sort by well name and date\n",
        "        self.wells_data = self.wells_data.sort_values(['WELL_NAME', 'PROD_DATE'])\n",
        "        \n",
        "        print(\"Data cleaning completed!\")\n",
        "        print(f\"Final dataset shape: {self.wells_data.shape}\")\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkosi-da-p/SPE_DATATHON/blob/main/DataPhandas_DSEATS_Africa_2025_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hWm6K-CwfNCa"
      },
      "outputs": [],
      "source": [
        "    def feature_engineering(self):\n",
        "            \"\"\"Engineer features from time series production data\"\"\"\n",
        "            print(\"\\nEngineering features...\")\n",
        "            \n",
        "            features_list = []\n",
        "            \n",
        "            for well in self.wells_data['WELL_NAME'].unique():\n",
        "                well_data = self.wells_data[self.wells_data['WELL_NAME'] == well].copy()\n",
        "                well_data = well_data.sort_values('PROD_DATE')\n",
        "                \n",
        "                # Calculate daily production rates\n",
        "                well_data['Daily_Oil_Prod'] = well_data['CUMULATIVE_OIL_PROD (STB)'].diff().fillna(0)\n",
        "                well_data['Daily_Gas_Prod'] = well_data['CUMULATIVE_FORMATION_GAS_PROD (MSCF)'].diff().fillna(0)\n",
        "                well_data['Daily_Water_Prod'] = well_data['CUMULATIVE_WATER_PROD (BBL)'].diff().fillna(0)\n",
        "                \n",
        "                # Calculate GOR (Gas Oil Ratio)\n",
        "                well_data['GOR'] = np.where(\n",
        "                    well_data['Daily_Oil_Prod'] > 0,\n",
        "                    well_data['Daily_Gas_Prod'] / well_data['Daily_Oil_Prod'] * 1000,  # Convert to SCF/STB\n",
        "                    0\n",
        "                )\n",
        "                \n",
        "                # Calculate water cut\n",
        "                total_liquid = well_data['Daily_Oil_Prod'] + well_data['Daily_Water_Prod']\n",
        "                well_data['Water_Cut'] = np.where(\n",
        "                    total_liquid > 0,\n",
        "                    well_data['Daily_Water_Prod'] / total_liquid * 100,\n",
        "                    0\n",
        "                )\n",
        "                \n",
        "                # Calculate productivity index (approximate)\n",
        "                avg_reservoir_pressure = 3500  # We'll refine this later\n",
        "                well_data['Productivity_Index'] = np.where(\n",
        "                    well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'] > 0,\n",
        "                    well_data['Daily_Oil_Prod'] / (avg_reservoir_pressure - well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)']),\n",
        "                    0\n",
        "                )\n",
        "                \n",
        "                # Identify reservoir for FVF lookup\n",
        "                reservoir_name = self._identify_reservoir({\n",
        "                    'Avg_BHP': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].mean()\n",
        "                })\n",
        "                \n",
        "                # Calculate reservoir barrels\n",
        "                cumulative_oil_stb = well_data['CUMULATIVE_OIL_PROD (STB)'].max()\n",
        "                cumulative_oil_rb = cumulative_oil_stb * self.reservoir_fvf[reservoir_name]\n",
        "                \n",
        "                # Aggregate features for the well\n",
        "                well_features = {\n",
        "                    'WELL_NAME': well,\n",
        "                    'Well_Number': int(well.split('_#')[1]),\n",
        "                    \n",
        "                    # Production statistics (both STB and RB)\n",
        "                    'Total_Oil_STB': cumulative_oil_stb,\n",
        "                    'Total_Oil_RB': cumulative_oil_rb,\n",
        "                    'Reservoir_FVF': self.reservoir_fvf[reservoir_name],\n",
        "                    'Total_Gas_Prod': well_data['CUMULATIVE_FORMATION_GAS_PROD (MSCF)'].max(),\n",
        "                    'Total_Water_Prod': well_data['CUMULATIVE_WATER_PROD (BBL)'].max(),\n",
        "                    'Avg_Daily_Oil': well_data['Daily_Oil_Prod'].mean(),\n",
        "                    'Max_Daily_Oil': well_data['Daily_Oil_Prod'].max(),\n",
        "                    'Oil_Production_Decline': self._calculate_decline_rate(well_data['Daily_Oil_Prod']),\n",
        "                    \n",
        "                    # Pressure statistics\n",
        "                    'Avg_BHP': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].mean(),\n",
        "                    'Max_BHP': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].max(),\n",
        "                    'Min_BHP': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].min(),\n",
        "                    'BHP_Variance': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].var(),\n",
        "                    'Avg_WHP': well_data['WELL_HEAD_PRESSURE (PSI)'].mean(),\n",
        "                    'Avg_Annulus_Press': well_data['ANNULUS_PRESS (PSI)'].mean(),\n",
        "                    \n",
        "                    # Temperature statistics\n",
        "                    'Avg_Downhole_Temp': well_data['DOWNHOLE_TEMPERATURE (deg F)'].mean(),\n",
        "                    'Avg_Wellhead_Temp': well_data['WELL_HEAD_TEMPERATURE (deg F)'].mean(),\n",
        "                    \n",
        "                    # Flow characteristics\n",
        "                    'Avg_Choke_Size': well_data['CHOKE_SIZE (%)'].mean(),\n",
        "                    'Choke_Variance': well_data['CHOKE_SIZE (%)'].var(),\n",
        "                    'Avg_Onstream_Hours': well_data['ON_STREAM_HRS'].mean(),\n",
        "                    \n",
        "                    # GOR and Water Cut trends\n",
        "                    'Avg_GOR': well_data['GOR'].mean(),\n",
        "                    'GOR_Trend': self._calculate_trend(well_data['GOR']),\n",
        "                    'Max_GOR': well_data['GOR'].max(),\n",
        "                    'Avg_Water_Cut': well_data['Water_Cut'].mean(),\n",
        "                    'Water_Cut_Trend': self._calculate_trend(well_data['Water_Cut']),\n",
        "                    'Max_Water_Cut': well_data['Water_Cut'].max(),\n",
        "                    \n",
        "                    # Productivity Index\n",
        "                    'Avg_PI': well_data['Productivity_Index'].mean(),\n",
        "                    'PI_Trend': self._calculate_trend(well_data['Productivity_Index']),\n",
        "                    'Max_PI': well_data['Productivity_Index'].max(),\n",
        "                    \n",
        "                    # Production stability\n",
        "                    'Production_Stability': self._calculate_stability(well_data['Daily_Oil_Prod']),\n",
        "                    'Days_Produced': len(well_data[well_data['Daily_Oil_Prod'] > 0]),\n",
        "                    'Production_Efficiency': well_data['ON_STREAM_HRS'].mean() / 24.0,\n",
        "                    \n",
        "                    # Well type indicators\n",
        "                    'Has_Annulus_Pressure': (well_data['ANNULUS_PRESS (PSI)'] > 0).any(),\n",
        "                    'Pressure_Differential': well_data['WELL_HEAD_PRESSURE (PSI)'].mean() - well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].mean(),\n",
        "                    \n",
        "                    # Reservoir info\n",
        "                    'Reservoir_Name': reservoir_name\n",
        "                }\n",
        "                \n",
        "                features_list.append(well_features)\n",
        "            \n",
        "            self.processed_features = pd.DataFrame(features_list)\n",
        "            print(f\"Feature engineering completed! Shape: {self.processed_features.shape}\")\n",
        "            print(f\"Features created: {len(self.processed_features.columns)}\")\n",
        "            \n",
        "    def calculate_reservoir_production(self):\n",
        "            \"\"\"Calculate total oil produced per reservoir in reservoir barrels (RB)\"\"\"\n",
        "            if not hasattr(self, 'processed_features'):\n",
        "                raise ValueError(\"Run feature_engineering() first\")\n",
        "                \n",
        "            # Group by reservoir and sum production\n",
        "            reservoir_production = self.processed_features.groupby('Reservoir_Name').agg({\n",
        "                'Total_Oil_STB': 'sum',\n",
        "                'Total_Oil_RB': 'sum'\n",
        "            }).rename(columns={\n",
        "                'Total_Oil_STB': 'Total_Surface_Barrels',\n",
        "                'Total_Oil_RB': 'Total_Reservoir_Barrels'\n",
        "            })\n",
        "            \n",
        "            return reservoir_production\n",
        "\n",
        "    def _calculate_decline_rate(self, production_series):\n",
        "            \"\"\"Calculate production decline rate\"\"\"\n",
        "            if len(production_series) < 2:\n",
        "                return 0\n",
        "            \n",
        "            # Use linear regression to estimate decline\n",
        "            x = np.arange(len(production_series))\n",
        "            y = production_series.values\n",
        "            \n",
        "            # Remove zeros for log calculation\n",
        "            y_nonzero = y[y > 0]\n",
        "            if len(y_nonzero) < 2:\n",
        "                return 0\n",
        "            \n",
        "            # Simple linear decline\n",
        "            if len(y_nonzero) == len(y):\n",
        "                slope = np.polyfit(x, y, 1)[0]\n",
        "                return slope / np.mean(y) if np.mean(y) > 0 else 0\n",
        "            else:\n",
        "                return 0\n",
        "        \n",
        "    def _calculate_trend(self, series):\n",
        "            \"\"\"Calculate trend direction: 1 for increasing, -1 for decreasing, 0 for flat\"\"\"\n",
        "            if len(series) < 2:\n",
        "                return 0\n",
        "            \n",
        "            # Remove outliers using IQR\n",
        "            Q1 = series.quantile(0.25)\n",
        "            Q3 = series.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            filtered_series = series[(series >= Q1 - 1.5*IQR) & (series <= Q3 + 1.5*IQR)]\n",
        "            \n",
        "            if len(filtered_series) < 2:\n",
        "                return 0\n",
        "            \n",
        "            # Calculate trend using linear regression\n",
        "            x = np.arange(len(filtered_series))\n",
        "            slope = np.polyfit(x, filtered_series.values, 1)[0]\n",
        "            \n",
        "            # Threshold for significance\n",
        "            threshold = 0.01 * np.mean(filtered_series)\n",
        "            \n",
        "            if slope > threshold:\n",
        "                return 1  # Increasing\n",
        "            elif slope < -threshold:\n",
        "                return -1  # Decreasing\n",
        "            else:\n",
        "                return 0  # Flat\n",
        "        \n",
        "    def _calculate_stability(self, production_series):\n",
        "            \"\"\"Calculate production stability (coefficient of variation)\"\"\"\n",
        "            if len(production_series) < 2 or production_series.mean() == 0:\n",
        "                return 0\n",
        "            \n",
        "            # Calculate coefficient of variation\n",
        "            cv = production_series.std() / production_series.mean()\n",
        "            return 1 / (1 + cv)  # Normalize to 0-1 where 1 is most stable\n",
        "        \n",
        "    def prepare_target_variables(self):\n",
        "            \"\"\"Prepare target variables for classification\"\"\"\n",
        "            print(\"\\nPreparing target variables...\")\n",
        "            \n",
        "            # Create a mapping from well number to classifications\n",
        "            target_df = pd.DataFrame({\n",
        "                'Well_Number': range(1, 21),\n",
        "                'Reservoir Name': None,\n",
        "                'Reservoir Type': None,\n",
        "                'Well Type': None,\n",
        "                'Production Type': None,\n",
        "                'Formation GOR Trend': None,\n",
        "                'Watercut Trend': None,\n",
        "                'Oil Productivity Index Trend': None\n",
        "            })\n",
        "            \n",
        "            # Implement classification logic based on reservoir engineering rules\n",
        "            for idx, row in self.processed_features.iterrows():\n",
        "                well_num = row['Well_Number']\n",
        "                \n",
        "                # Reservoir identification based on BHP and reservoir pressures\n",
        "                reservoir_name = self._identify_reservoir(row)\n",
        "                target_df.loc[well_num-1, 'Reservoir Name'] = reservoir_name\n",
        "                \n",
        "                # Reservoir type based on reservoir info\n",
        "                reservoir_type = self._determine_reservoir_type(reservoir_name)\n",
        "                target_df.loc[well_num-1, 'Reservoir Type'] = reservoir_type\n",
        "                \n",
        "                # Well type based on annulus pressure\n",
        "                well_type = 'GL' if row['Has_Annulus_Pressure'] else 'NF'\n",
        "                target_df.loc[well_num-1, 'Well Type'] = well_type\n",
        "                \n",
        "                # Production type based on stability\n",
        "                prod_type = 'Steady' if row['Production_Stability'] > 0.5 else 'Unsteady'\n",
        "                target_df.loc[well_num-1, 'Production Type'] = prod_type\n",
        "                \n",
        "                # GOR trend classification\n",
        "                gor_trend = self._classify_gor_trend(row, reservoir_name)\n",
        "                target_df.loc[well_num-1, 'Formation GOR Trend'] = gor_trend\n",
        "                \n",
        "                # Water cut trend\n",
        "                wc_trend = self._classify_watercut_trend(row['Water_Cut_Trend'])\n",
        "                target_df.loc[well_num-1, 'Watercut Trend'] = wc_trend\n",
        "                \n",
        "                # PI trend\n",
        "                pi_trend = self._classify_pi_trend(row['PI_Trend'])\n",
        "                target_df.loc[well_num-1, 'Oil Productivity Index Trend'] = pi_trend\n",
        "            \n",
        "            self.target_data = target_df\n",
        "            print(\"Target variables prepared!\")\n",
        "            \n",
        "    def _identify_reservoir(self, well_features):\n",
        "            \"\"\"Identify reservoir based on pressure characteristics\"\"\"\n",
        "            bhp = well_features['Avg_BHP']\n",
        "            \n",
        "            # Reservoir pressure ranges (with 200 psi differential allowance)\n",
        "            reservoir_pressures = {\n",
        "                'ACHI': 2700,\n",
        "                'KEMA': 3900,\n",
        "                'MAKO': 3000,\n",
        "                'DEPU': 2400,\n",
        "                'JANI': 4200\n",
        "            }\n",
        "            \n",
        "            # Find closest match within 200 psi\n",
        "            min_diff = float('inf')\n",
        "            best_reservoir = 'ACHI'\n",
        "            \n",
        "            for reservoir, pressure in reservoir_pressures.items():\n",
        "                diff = abs(bhp - pressure)\n",
        "                if diff < min_diff and diff <= 200:\n",
        "                    min_diff = diff\n",
        "                    best_reservoir = reservoir\n",
        "            \n",
        "            return best_reservoir\n",
        "        \n",
        "    def _determine_reservoir_type(self, reservoir_name):\n",
        "            \"\"\"Determine if reservoir is saturated or undersaturated\"\"\"\n",
        "            reservoir_types = {\n",
        "                'ACHI': 'Saturated',\n",
        "                'KEMA': 'Undersat',\n",
        "                'MAKO': 'Saturated',\n",
        "                'DEPU': 'Saturated',\n",
        "                'JANI': 'Undersat'\n",
        "            }\n",
        "            return reservoir_types.get(reservoir_name, 'Saturated')\n",
        "        \n",
        "    def _classify_gor_trend(self, well_features, reservoir_name):\n",
        "            \"\"\"Classify GOR trend relative to solution GOR\"\"\"\n",
        "            solution_gor = {\n",
        "                'ACHI': 800,\n",
        "                'KEMA': 600,\n",
        "                'MAKO': 500,\n",
        "                'DEPU': 1200,\n",
        "                'JANI': 1000\n",
        "            }\n",
        "            \n",
        "            avg_gor = well_features['Avg_GOR']\n",
        "            sol_gor = solution_gor.get(reservoir_name, 800)\n",
        "            \n",
        "            if avg_gor > sol_gor * 1.1:\n",
        "                return 'aSolGOR'\n",
        "            elif avg_gor < sol_gor * 0.9:\n",
        "                return 'bSolGOR'\n",
        "            else:\n",
        "                return 'Combo'\n",
        "        \n",
        "    def _classify_watercut_trend(self, trend_value):\n",
        "            \"\"\"Classify water cut trend\"\"\"\n",
        "            if trend_value > 0.5:\n",
        "                return 'Incr'\n",
        "            elif trend_value < -0.5:\n",
        "                return 'Decr'\n",
        "            else:\n",
        "                return 'Flat'\n",
        "        \n",
        "    def _classify_pi_trend(self, trend_value):\n",
        "            \"\"\"Classify productivity index trend\"\"\"\n",
        "            if trend_value > 0.5:\n",
        "                return 'Incr'\n",
        "            elif trend_value < -0.5:\n",
        "                return 'Decr'\n",
        "            else:\n",
        "                return 'Flat'\n",
        "        \n",
        "    def prepare_features_for_ml(self):\n",
        "            \"\"\"Prepare features for machine learning\"\"\"\n",
        "            print(\"\\nPreparing features for ML...\")\n",
        "            \n",
        "            # Select numerical features for ML - CORRECTED COLUMN NAMES\n",
        "            feature_columns = [\n",
        "                'Total_Oil_STB',  # Changed from 'Total_Oil_Prod'\n",
        "                'Total_Gas_Prod', 'Total_Water_Prod',\n",
        "                'Avg_Daily_Oil', 'Max_Daily_Oil', 'Oil_Production_Decline',\n",
        "                'Avg_BHP', 'Max_BHP', 'Min_BHP', 'BHP_Variance',\n",
        "                'Avg_WHP', 'Avg_Annulus_Press',\n",
        "                'Avg_Downhole_Temp', 'Avg_Wellhead_Temp',\n",
        "                'Avg_Choke_Size', 'Choke_Variance', 'Avg_Onstream_Hours',\n",
        "                'Avg_GOR', 'GOR_Trend', 'Max_GOR',\n",
        "                'Avg_Water_Cut', 'Water_Cut_Trend', 'Max_Water_Cut',\n",
        "                'Avg_PI', 'PI_Trend', 'Max_PI',\n",
        "                'Production_Stability', 'Days_Produced', 'Production_Efficiency',\n",
        "                'Pressure_Differential'\n",
        "            ]\n",
        "            \n",
        "            # Convert boolean to numeric\n",
        "            self.processed_features['Has_Annulus_Pressure'] = self.processed_features['Has_Annulus_Pressure'].astype(int)\n",
        "            feature_columns.append('Has_Annulus_Pressure')\n",
        "            \n",
        "            # Debug: Print available columns vs requested columns\n",
        "            print(\"Available columns in processed_features:\")\n",
        "            print(self.processed_features.columns.tolist())\n",
        "            print(\"\\nRequested feature columns:\")\n",
        "            print(feature_columns)\n",
        "            \n",
        "            # Check which columns are missing\n",
        "            missing_columns = [col for col in feature_columns if col not in self.processed_features.columns]\n",
        "            if missing_columns:\n",
        "                print(f\"\\nMissing columns: {missing_columns}\")\n",
        "                # Remove missing columns from feature_columns\n",
        "                feature_columns = [col for col in feature_columns if col in self.processed_features.columns]\n",
        "                print(f\"Using available columns: {feature_columns}\")\n",
        "            \n",
        "            # Create feature matrix\n",
        "            X = self.processed_features[feature_columns].fillna(0)\n",
        "            \n",
        "            # Scale features\n",
        "            X_scaled = self.scaler.fit_transform(X)\n",
        "            self.X_scaled = pd.DataFrame(X_scaled, columns=feature_columns)\n",
        "            \n",
        "            print(f\"Features prepared! Shape: {self.X_scaled.shape}\")\n",
        "            return feature_columns  # Return the actual columns used\n",
        "\n",
        "    def generate_classification_results(self):\n",
        "            \"\"\"Generate final classification results and production summary\"\"\"\n",
        "            print(\"\\nGenerating classification results...\")\n",
        "            \n",
        "            # Make predictions\n",
        "            results = []\n",
        "            for well_num in range(1, 21):\n",
        "                well_result = {'Well': well_num}\n",
        "                features = self.X_scaled[self.X_scaled.index == well_num - 1]\n",
        "                \n",
        "                for target_col, model in self.models.items():\n",
        "                    le = self.label_encoders[target_col]\n",
        "                    pred = model.predict(features)[0]\n",
        "                    well_result[target_col] = le.inverse_transform([pred])[0]\n",
        "                \n",
        "                results.append(well_result)\n",
        "            \n",
        "            results_df = pd.DataFrame(results)\n",
        "            \n",
        "            # Calculate reservoir production totals\n",
        "            reservoir_prod = self.calculate_reservoir_production()\n",
        "            \n",
        "            # Save results\n",
        "            results_df.to_csv('Data_Phandas_DSEATS_Africa_2025_Classification.csv', index=False)\n",
        "            reservoir_prod.to_csv('Data_Phandas_DSEATS_Africa_2025_ReservoirProduction.csv', index=False)\n",
        "            \n",
        "            print(\"Classification results generated and saved!\")\n",
        "            print(\"\\nReservoir Production Summary:\")\n",
        "            print(reservoir_prod)\n",
        "            \n",
        "            return results_df, reservoir_prod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def train_xgboost_model(self):\n",
        "            \"\"\"Train XGBoost model with hyperparameter tuning with enhanced evaluation\"\"\"\n",
        "            print(\"\\nTraining XGBoost model with hyperparameter tuning...\")\n",
        "\n",
        "            # Prepare target variables\n",
        "            y_encoded = {}\n",
        "            valid_targets = []\n",
        "\n",
        "            # First pass to check which targets have multiple classes\n",
        "            for target_col in self.target_columns:\n",
        "                le = LabelEncoder()\n",
        "                y_encoded[target_col] = le.fit_transform(self.target_data[target_col].astype(str))\n",
        "                self.label_encoders[target_col] = le\n",
        "                \n",
        "                if len(np.unique(y_encoded[target_col])) > 1:\n",
        "                    valid_targets.append(target_col)\n",
        "                else:\n",
        "                    print(f\"Skipping {target_col} - only one class present\")\n",
        "\n",
        "            if not valid_targets:\n",
        "                raise ValueError(\"No valid targets with multiple classes for classification\")\n",
        "\n",
        "            # Enhanced parameter space for tuning\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 150, 200, 250],\n",
        "                'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "                'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
        "                'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "                'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "                'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
        "                'min_child_weight': [1, 3, 5],\n",
        "                'reg_alpha': [0, 0.1, 0.5, 1],\n",
        "                'reg_lambda': [0.5, 1, 1.5, 2]\n",
        "            }\n",
        "\n",
        "            # Train individual models for each target\n",
        "            self.models = {}\n",
        "            \n",
        "            for target_col in valid_targets:\n",
        "                print(f\"\\nTuning XGBoost for target: {target_col}\")\n",
        "                \n",
        "                n_classes = len(np.unique(y_encoded[target_col]))\n",
        "                class_names = self.label_encoders[target_col].classes_\n",
        "                \n",
        "                model = XGBClassifier(\n",
        "                    objective='multi:softprob' if n_classes > 2 else 'binary:logistic',\n",
        "                    num_class=n_classes if n_classes > 2 else None,\n",
        "                    eval_metric='mlogloss' if n_classes > 2 else 'logloss',\n",
        "                    use_label_encoder=False,\n",
        "                    verbosity=0,\n",
        "                    random_state=42,\n",
        "                    early_stopping_rounds=10\n",
        "                )\n",
        "\n",
        "                # Use stratified k-fold for classification\n",
        "                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "                \n",
        "                # Compute sample weights for class imbalance\n",
        "                sample_weights = compute_sample_weight('balanced', y_encoded[target_col])\n",
        "                \n",
        "                # Randomized search for hyperparameter tuning with more iterations\n",
        "                search = RandomizedSearchCV(\n",
        "                    model,\n",
        "                    param_distributions=param_grid,\n",
        "                    n_iter=50,  # Increased from 20 to 50 for better exploration\n",
        "                    cv=cv,\n",
        "                    scoring='accuracy',\n",
        "                    random_state=42,\n",
        "                    n_jobs=-1,\n",
        "                    verbose=1\n",
        "                )\n",
        "\n",
        "                search.fit(self.X_scaled, y_encoded[target_col], \n",
        "                        sample_weight=sample_weights,\n",
        "                        eval_set=[(self.X_scaled, y_encoded[target_col])],\n",
        "                        verbose=False)\n",
        "                \n",
        "                self.models[target_col] = search.best_estimator_\n",
        "                self.best_params[target_col] = search.best_params_\n",
        "\n",
        "                print(f\"Best params for {target_col}: {search.best_params_}\")\n",
        "                print(f\"Best CV score: {search.best_score_:.3f}\")\n",
        "\n",
        "                # Enhanced evaluation on training data\n",
        "                y_pred = self.models[target_col].predict(self.X_scaled)\n",
        "                y_proba = self.models[target_col].predict_proba(self.X_scaled)\n",
        "                \n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_encoded[target_col], y_pred)\n",
        "                f1 = f1_score(y_encoded[target_col], y_pred, average='weighted')\n",
        "                cm = confusion_matrix(y_encoded[target_col], y_pred)\n",
        "                \n",
        "                print(f\"\\nEvaluation for {target_col}:\")\n",
        "                print(f\"Accuracy: {accuracy:.4f}\")\n",
        "                print(f\"Weighted F1 Score: {f1:.4f}\")\n",
        "                print(\"\\nConfusion Matrix:\")\n",
        "                print(cm)\n",
        "                \n",
        "                # Print classification report\n",
        "                print(\"\\nClassification Report:\")\n",
        "                print(classification_report(y_encoded[target_col], y_pred, target_names=class_names))\n",
        "                \n",
        "                # Add confidence analysis\n",
        "                max_proba = np.max(y_proba, axis=1)\n",
        "                print(f\"\\nPrediction Confidence Analysis:\")\n",
        "                print(f\"Average confidence for correct predictions: {np.mean(max_proba[y_pred == y_encoded[target_col]]):.4f}\")\n",
        "                print(f\"Average confidence for incorrect predictions: {np.mean(max_proba[y_pred != y_encoded[target_col]]):.4f}\")\n",
        "                \n",
        "                # Store evaluation metrics\n",
        "                self.models[target_col].training_metrics = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'f1_score': f1,\n",
        "                    'confusion_matrix': cm,\n",
        "                    'classification_report': classification_report(y_encoded[target_col], y_pred, target_names=class_names, output_dict=True)\n",
        "                }\n",
        "\n",
        "            # Overall evaluation\n",
        "            accuracies = {}\n",
        "            f1_scores = {}\n",
        "            for target_col in valid_targets:\n",
        "                y_pred = self.models[target_col].predict(self.X_scaled)\n",
        "                accuracies[target_col] = accuracy_score(y_encoded[target_col], y_pred)\n",
        "                f1_scores[target_col] = f1_score(y_encoded[target_col], y_pred, average='weighted')\n",
        "\n",
        "            print(\"\\nOverall Performance Summary:\")\n",
        "            print(f\"Mean accuracy: {np.mean(list(accuracies.values())):.4f}\")\n",
        "            print(f\"Mean weighted F1 score: {np.mean(list(f1_scores.values())):.4f}\")\n",
        "\n",
        "            # Feature importance analysis\n",
        "            for target_col in valid_targets:\n",
        "                print(f\"\\nFeature Importance for {target_col}:\")\n",
        "                importances = self.models[target_col].feature_importances_\n",
        "                indices = np.argsort(importances)[::-1]\n",
        "                \n",
        "                print(\"Top 10 Important Features:\")\n",
        "                for i in range(min(10, len(indices))):\n",
        "                    print(f\"{i+1}. Feature {indices[i]} - Importance: {importances[indices[i]]:.4f}\")\n",
        "                \n",
        "    def explain_xgboost_model(self):\n",
        "            \"\"\"Explain XGBoost model and feature importance\"\"\"\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"XGBOOST MODEL EXPLANATION\")\n",
        "            print(\"=\"*50)\n",
        "            \n",
        "            print(\"\"\"\n",
        "            XGBOOST OVERVIEW:\n",
        "            ================\n",
        "            \n",
        "            XGBoost (eXtreme Gradient Boosting) is an optimized gradient boosting framework\n",
        "            that uses an ensemble of decision trees. It builds models sequentially, where\n",
        "            each new model corrects errors made by previous models.\n",
        "            \n",
        "            MATHEMATICAL FOUNDATION:\n",
        "            ======================\n",
        "            \n",
        "            XGBoost minimizes the objective function:\n",
        "            Obj = Σ L(yi, ŷi) + Σ Ω(fk)\n",
        "            \n",
        "            Where:\n",
        "            - L(yi, ŷi) is the loss function (e.g., logistic loss for classification)\n",
        "            - Ω(fk) is the regularization term to prevent overfitting\n",
        "            - fk represents individual trees\n",
        "            \n",
        "            KEY FEATURES:\n",
        "            ============\n",
        "            1. Gradient Boosting: Sequential model improvement\n",
        "            2. Regularization: L1 and L2 regularization to prevent overfitting\n",
        "            3. Tree Pruning: Removes unnecessary branches\n",
        "            4. Parallel Processing: Efficient computation\n",
        "            5. Missing Value Handling: Automatic handling of missing values\n",
        "            \n",
        "            HYPERPARAMETERS TUNED:\n",
        "            =====================\n",
        "            \"\"\")\n",
        "            \n",
        "            # Display best parameters for each target\n",
        "            for target_col, params in self.best_params.items():\n",
        "                print(f\"\\n{target_col}:\")\n",
        "                for param, value in params.items():\n",
        "                    print(f\"  - {param}: {value}\")\n",
        "            \n",
        "            print(f\"\"\"\n",
        "            \n",
        "            ADVANTAGES FOR WELL CLASSIFICATION:\n",
        "            =================================\n",
        "            1. Handles non-linear relationships between features\n",
        "            2. Automatic feature selection through tree splits\n",
        "            3. Robust to outliers and missing values\n",
        "            4. Provides feature importance scores\n",
        "            5. Excellent performance on tabular data\n",
        "            6. Handles mixed data types (numerical and categorical)\n",
        "            \n",
        "            FEATURE IMPORTANCE:\n",
        "            ==================\n",
        "            \"\"\")\n",
        "            \n",
        "            # Calculate and display feature importance\n",
        "            self._display_feature_importance()\n",
        "            \n",
        "    def _display_feature_importance(self):\n",
        "            \"\"\"Display feature importance for each target\"\"\"\n",
        "            feature_names = self.X_scaled.columns\n",
        "            \n",
        "            for target_col, model in self.models.items():\n",
        "                print(f\"\\nTop 10 Important Features for {target_col}:\")\n",
        "                \n",
        "                if hasattr(model, 'feature_importances_'):\n",
        "                    importances = model.feature_importances_\n",
        "                    \n",
        "                    # Create feature importance dataframe\n",
        "                    importance_df = pd.DataFrame({\n",
        "                        'feature': feature_names,\n",
        "                        'importance': importances\n",
        "                    }).sort_values('importance', ascending=False)\n",
        "                    \n",
        "                    # Display top 10 features\n",
        "                    for j, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
        "                        print(f\"  {j+1}. {row['feature']}: {row['importance']:.4f}\")\n",
        "        \n",
        "    def generate_classification_results(self):\n",
        "            \"\"\"Generate final classification results\"\"\"\n",
        "            print(\"\\nGenerating classification results...\")\n",
        "            \n",
        "            # Initialize results dataframe\n",
        "            results = []\n",
        "            \n",
        "            for well_num in self.processed_features['Well_Number']:\n",
        "                well_result = {'Well': well_num}\n",
        "                features = self.X_scaled[self.X_scaled.index == well_num - 1]\n",
        "                \n",
        "                for target_col, model in self.models.items():\n",
        "                    le = self.label_encoders[target_col]\n",
        "                    \n",
        "                    # Predict and decode\n",
        "                    pred = model.predict(features)[0]\n",
        "                    well_result[target_col] = le.inverse_transform([pred])[0]\n",
        "                    \n",
        "                results.append(well_result)\n",
        "            \n",
        "            results_df = pd.DataFrame(results)\n",
        "            \n",
        "            # Save results\n",
        "            results_df.to_csv('Data_Phandas1_DSEATS_Africa_2025_XGBoost_Classification.csv', index=False)\n",
        "            \n",
        "            print(\"Classification results generated and saved!\")\n",
        "            print(results_df.head())\n",
        "            \n",
        "            return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "    def visualize_results(self):\n",
        "            \"\"\"Create visualizations including reservoir production\"\"\"\n",
        "            # Set up the plotting\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "            \n",
        "            # Plot 1: Production Profile by Well\n",
        "            ax1 = axes[0, 0]\n",
        "            for well in self.processed_features['Well_Number'].head(5):\n",
        "                well_data = self.wells_data[self.wells_data['WELL_NAME'] == f'WELL_#{well}']\n",
        "                ax1.plot(well_data['PROD_DATE'], well_data['CUMULATIVE_OIL_PROD (STB)'], \n",
        "                        label=f'Well #{well}', linewidth=2)\n",
        "            ax1.set_title('Cumulative Oil Production by Well', fontsize=14, fontweight='bold')\n",
        "            ax1.set_xlabel('Date')\n",
        "            ax1.set_ylabel('Cumulative Oil (STB)')\n",
        "            ax1.legend()\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Plot 2: Pressure Distribution\n",
        "            ax2 = axes[0, 1]\n",
        "            ax2.boxplot([self.processed_features['Avg_BHP'], \n",
        "                        self.processed_features['Avg_WHP']], \n",
        "                    labels=['BHP', 'WHP'])\n",
        "            ax2.set_title('Pressure Distribution', fontsize=14, fontweight='bold')\n",
        "            ax2.set_ylabel('Pressure (PSI)')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Plot 3: GOR vs Water Cut\n",
        "            ax3 = axes[0, 2]\n",
        "            scatter = ax3.scatter(self.processed_features['Avg_GOR'], \n",
        "                                self.processed_features['Avg_Water_Cut'],\n",
        "                                c=self.processed_features['Well_Number'], \n",
        "                                cmap='viridis', s=100, alpha=0.7)\n",
        "            ax3.set_title('GOR vs Water Cut', fontsize=14, fontweight='bold')\n",
        "            ax3.set_xlabel('Average GOR (SCF/STB)')\n",
        "            ax3.set_ylabel('Average Water Cut (%)')\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "            plt.colorbar(scatter, ax=ax3, label='Well Number')\n",
        "            \n",
        "            # Plot 4: Production Stability\n",
        "            ax4 = axes[1, 0]\n",
        "            ax4.hist(self.processed_features['Production_Stability'], \n",
        "                    bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            ax4.set_title('Production Stability Distribution', fontsize=14, fontweight='bold')\n",
        "            ax4.set_xlabel('Production Stability Index')\n",
        "            ax4.set_ylabel('Frequency')\n",
        "            ax4.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Plot 5: Reservoir Classification\n",
        "            ax5 = axes[1, 1]\n",
        "            reservoir_counts = self.target_data['Reservoir Name'].value_counts()\n",
        "            ax5.pie(reservoir_counts.values, labels=reservoir_counts.index, \n",
        "                autopct='%1.1f%%', startangle=90)\n",
        "            ax5.set_title('Reservoir Distribution', fontsize=14, fontweight='bold')\n",
        "            \n",
        "            # Plot 6: Feature Correlation Heatmap\n",
        "            ax6 = axes[1, 2]\n",
        "            important_features = ['Avg_BHP', 'Avg_GOR', 'Avg_Water_Cut', 'Production_Stability', \n",
        "                                'Avg_Daily_Oil', 'Pressure_Differential']\n",
        "            \n",
        "            corr_matrix = self.processed_features[important_features].corr()\n",
        "            im = ax6.imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
        "            ax6.set_xticks(range(len(important_features)))\n",
        "            ax6.set_yticks(range(len(important_features)))\n",
        "            ax6.set_xticklabels([feat.replace('_', ' ') for feat in important_features], \n",
        "                            rotation=45, ha='right')\n",
        "            ax6.set_yticklabels([feat.replace('_', ' ') for feat in important_features])\n",
        "            ax6.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "            \n",
        "            # Add correlation values to heatmap\n",
        "            for i in range(len(important_features)):\n",
        "                for j in range(len(important_features)):\n",
        "                    text = ax6.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
        "                                ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "            \n",
        "            # Add colorbar\n",
        "            cbar = plt.colorbar(im, ax=ax6, shrink=0.8)\n",
        "            cbar.set_label('Correlation Coefficient', rotation=270, labelpad=20)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig('well_classification_analysis.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "            # Create additional detailed visualizations\n",
        "            self._create_detailed_visualizations()\n",
        "            \n",
        "            print(\"Visualizations created and saved!\")\n",
        "        \n",
        "    def _create_detailed_visualizations(self):\n",
        "            \"\"\"Create additional detailed visualizations\"\"\"\n",
        "            \n",
        "            # Classification Results Summary\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            \n",
        "            # Well Type Distribution\n",
        "            ax1 = axes[0, 0]\n",
        "            well_type_counts = self.target_data['Well Type'].value_counts()\n",
        "            colors = ['#FF9999', '#66B2FF']\n",
        "            ax1.pie(well_type_counts.values, labels=well_type_counts.index, \n",
        "                autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "            ax1.set_title('Well Type Distribution', fontsize=14, fontweight='bold')\n",
        "            \n",
        "            # Production Type Distribution\n",
        "            ax2 = axes[0, 1]\n",
        "            prod_type_counts = self.target_data['Production Type'].value_counts()\n",
        "            colors = ['#99FF99', '#FFB366']\n",
        "            ax2.pie(prod_type_counts.values, labels=prod_type_counts.index, \n",
        "                autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "            ax2.set_title('Production Type Distribution', fontsize=14, fontweight='bold')\n",
        "            \n",
        "            # GOR Trend Classification\n",
        "            ax3 = axes[1, 0]\n",
        "            gor_trend_counts = self.target_data['Formation GOR Trend'].value_counts()\n",
        "            ax3.bar(gor_trend_counts.index, gor_trend_counts.values, \n",
        "                color=['#FF99CC', '#99CCFF', '#FFFF99'])\n",
        "            ax3.set_title('Formation GOR Trend Classification', fontsize=14, fontweight='bold')\n",
        "            ax3.set_xlabel('GOR Trend')\n",
        "            ax3.set_ylabel('Number of Wells')\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Water Cut Trend\n",
        "            ax4 = axes[1, 1]\n",
        "            wc_trend_counts = self.target_data['Watercut Trend'].value_counts()\n",
        "            ax4.bar(wc_trend_counts.index, wc_trend_counts.values, \n",
        "                color=['#FFB3BA', '#BAFFC9', '#BAE1FF'])\n",
        "            ax4.set_title('Water Cut Trend Classification', fontsize=14, fontweight='bold')\n",
        "            ax4.set_xlabel('Water Cut Trend')\n",
        "            ax4.set_ylabel('Number of Wells')\n",
        "            ax4.grid(True, alpha=0.3)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig('classification_results_summary.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "            # Model Performance Visualization\n",
        "            self._plot_model_performance()\n",
        "        \n",
        "    def _plot_model_performance(self):\n",
        "            \"\"\"Plot model performance metrics\"\"\"\n",
        "            \n",
        "            # Create performance summary\n",
        "            accuracies = []\n",
        "            for target_col, model in self.models.items():\n",
        "                y_true = self.label_encoders[target_col].transform(self.target_data[target_col])\n",
        "                y_pred = model.predict(self.X_scaled)\n",
        "                acc = accuracy_score(y_true, y_pred)\n",
        "                accuracies.append(acc)\n",
        "            \n",
        "            # Plot accuracy by target\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "            \n",
        "            # Accuracy bar chart\n",
        "            colors = plt.cm.Set3(np.linspace(0, 1, len(self.models)))\n",
        "            bars = ax1.bar(range(len(self.models)), accuracies, color=colors)\n",
        "            ax1.set_xlabel('Classification Target')\n",
        "            ax1.set_ylabel('Accuracy')\n",
        "            ax1.set_title('XGBoost Model Accuracy by Target', fontsize=14, fontweight='bold')\n",
        "            ax1.set_xticks(range(len(self.models)))\n",
        "            ax1.set_xticklabels([col.replace(' ', '\\n') for col in self.models.keys()], \n",
        "                            rotation=45, ha='right')\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add value labels on bars\n",
        "            for bar, acc in zip(bars, accuracies):\n",
        "                height = bar.get_height()\n",
        "                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                        f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "            \n",
        "            # Feature importance summary (average across all targets)\n",
        "            feature_names = self.X_scaled.columns\n",
        "            avg_importance = np.zeros(len(feature_names))\n",
        "            \n",
        "            for model in self.models.values():\n",
        "                if hasattr(model, 'feature_importances_'):\n",
        "                    avg_importance += model.feature_importances_\n",
        "            \n",
        "            avg_importance /= len(self.models)\n",
        "            \n",
        "            # Plot top 10 features\n",
        "            top_indices = np.argsort(avg_importance)[-10:]\n",
        "            top_features = [feature_names[i] for i in top_indices]\n",
        "            top_importance = avg_importance[top_indices]\n",
        "            \n",
        "            ax2.barh(range(len(top_features)), top_importance, color='skyblue')\n",
        "            ax2.set_yticks(range(len(top_features)))\n",
        "            ax2.set_yticklabels([feat.replace('_', ' ') for feat in top_features])\n",
        "            ax2.set_xlabel('Average Feature Importance')\n",
        "            ax2.set_title('Top 10 Most Important Features', fontsize=14, fontweight='bold')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig('model_performance_summary.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()    \n",
        "            \n",
        "            # Add reservoir production plot\n",
        "            reservoir_prod = self.calculate_reservoir_production()\n",
        "            \n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            reservoir_prod['Total_Reservoir_Barrels'].plot(\n",
        "                kind='bar', \n",
        "                color='darkorange',\n",
        "                ax=ax,\n",
        "                title='Total Oil Production by Reservoir (Reservoir Barrels)'\n",
        "            )\n",
        "            ax.set_ylabel('Barrels (RB)')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('reservoir_production_summary.png', dpi=300)\n",
        "            plt.show()\n",
        "\n",
        "    def save_model_and_results(self):\n",
        "            \"\"\"Save the trained model and all results\"\"\"\n",
        "            print(\"\\nSaving model and results...\")\n",
        "        \n",
        "        # Save the trained model\n",
        "            model_data = {\n",
        "                'model': self.models,\n",
        "                'scaler': self.scaler,\n",
        "                'label_encoders': self.label_encoders,\n",
        "                'best_params': self.best_params,\n",
        "                'feature_columns': self.X_scaled.columns.tolist(),\n",
        "                'target_columns': self.target_columns\n",
        "            }\n",
        "            \n",
        "            with open('xgboost_well_classification_model.pkl', 'wb') as f:\n",
        "                pickle.dump(model_data, f)\n",
        "            \n",
        "            # Save processed features\n",
        "            self.processed_features.to_csv('processed_well_features.csv', index=False)\n",
        "            \n",
        "            # Save target data\n",
        "            self.target_data.to_csv('target_classifications.csv', index=False)\n",
        "            \n",
        "            # Create model summary report\n",
        "            self._create_model_report()\n",
        "            \n",
        "            print(\"Model and results saved successfully!\")\n",
        "        \n",
        "    def _create_model_report(self):\n",
        "            \"\"\"Create a comprehensive model report\"\"\"\n",
        "            \n",
        "            report = f\"\"\"\n",
        "        ================================================================================\n",
        "                                XGBOOST WELL CLASSIFICATION MODEL REPORT\n",
        "        ================================================================================\n",
        "\n",
        "        MODEL OVERVIEW:\n",
        "        ==============\n",
        "        - Algorithm: XGBoost (eXtreme Gradient Boosting)\n",
        "        - Problem Type: Multi-output Classification\n",
        "        - Number of Wells: {len(self.processed_features)}\n",
        "        - Number of Features: {len(self.X_scaled.columns)}\n",
        "        - Number of Targets: {len(self.target_columns)}\n",
        "        - Targets Modeled: {len(self.models)}/{len(self.target_columns)}\n",
        "\n",
        "        CLASSIFICATION TARGETS:\n",
        "        ======================\n",
        "        \"\"\"\n",
        "            \n",
        "            for i, target in enumerate(self.target_columns, 1):\n",
        "                unique_classes = self.target_data[target].unique()\n",
        "                modeled = \"(Modeled)\" if target in self.models else \"(Skipped - single class)\"\n",
        "                report += f\"{i}. {target}: {len(unique_classes)} classes {list(unique_classes)} {modeled}\\n\"\n",
        "            \n",
        "            report += f\"\"\"\n",
        "\n",
        "        HYPERPARAMETER TUNING RESULTS:\n",
        "        ==============================\n",
        "        \"\"\"\n",
        "            \n",
        "            for target, params in self.best_params.items():\n",
        "                report += f\"\\n{target}:\\n\"\n",
        "                for param, value in params.items():\n",
        "                    report += f\"  - {param}: {value}\\n\"\n",
        "            \n",
        "            report += f\"\"\"\n",
        "\n",
        "        MODEL PERFORMANCE:\n",
        "        ==================\n",
        "        \"\"\"\n",
        "            \n",
        "            # Calculate and add performance metrics\n",
        "            accuracies = {}\n",
        "            for target_col, model in self.models.items():\n",
        "                y_true = self.label_encoders[target_col].transform(self.target_data[target_col])\n",
        "                y_pred = model.predict(self.X_scaled)\n",
        "                y_pred_labels = self.label_encoders[target_col].inverse_transform(y_pred)\n",
        "                acc = accuracy_score(y_true, y_pred)\n",
        "                report += f\"- {target_col}: {acc:.3f}\\n\"\n",
        "                accuracies[target_col] = acc\n",
        "            \n",
        "            if accuracies:\n",
        "                overall_acc = np.mean(list(accuracies.values()))\n",
        "                report += f\"\\nOverall Average Accuracy: {overall_acc:.3f}\\n\"\n",
        "            else:\n",
        "                report += \"\\nNo models were trained (all targets had only one class)\\n\"\n",
        "            \n",
        "            report += f\"\"\"\n",
        "\n",
        "        FEATURE ENGINEERING:\n",
        "        ===================\n",
        "        - Production statistics (cumulative and daily rates)\n",
        "        - Pressure characteristics (BHP, WHP, annulus pressure)\n",
        "        - Flow parameters (GOR, water cut, productivity index)\n",
        "        - Temperature measurements\n",
        "        - Production stability and efficiency metrics\n",
        "        - Trend analysis for key parameters\n",
        "\n",
        "        MODEL ADVANTAGES:\n",
        "        =================\n",
        "        1. Handles non-linear relationships between reservoir parameters\n",
        "        2. Automatic feature selection through tree-based splits\n",
        "        3. Robust to outliers and missing values in production data\n",
        "        4. Provides interpretable feature importance rankings\n",
        "        5. Excellent performance on tabular reservoir engineering data\n",
        "        6. Hyperparameter tuning for optimal performance\n",
        "\n",
        "        RECOMMENDATIONS:\n",
        "        ===============\n",
        "        1. Regular model retraining with new production data\n",
        "        2. Feature engineering refinement based on domain expertise\n",
        "        3. Cross-validation with additional well data for validation\n",
        "        4. Integration with reservoir simulation models\n",
        "        5. Continuous monitoring of model performance in production\n",
        "\n",
        "        FILES GENERATED:\n",
        "        ===============\n",
        "        - Team_DSEATS_Africa_2025_XGBoost_Classification.csv: Final classifications\n",
        "        - xgboost_well_classification_model.pkl: Trained model\n",
        "        - processed_well_features.csv: Engineered features\n",
        "        - target_classifications.csv: Target variable mappings\n",
        "        - well_classification_analysis.png: EDA visualizations\n",
        "        - classification_results_summary.png: Results summary\n",
        "        - model_performance_summary.png: Performance metrics\n",
        "\n",
        "        ================================================================================\n",
        "                                            END OF REPORT\n",
        "        ================================================================================\n",
        "        \"\"\"\n",
        "            \n",
        "            # Save report\n",
        "            with open('XGBoost_Well_Classification_Report.txt', 'w') as f:\n",
        "                f.write(report)\n",
        "            \n",
        "            print(\"Model report generated and saved!\")\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Run the complete pipeline\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"SPE DSEATS AFRICA 2025 - WELL CLASSIFICATION PIPELINE\")\n",
        "        print(\"=\"*60)\n",
        "            \n",
        "            # Execute pipeline steps\n",
        "        self.load_data()\n",
        "        self.clean_data()\n",
        "        self.feature_engineering()\n",
        "        self.prepare_target_variables()\n",
        "        self.prepare_features_for_ml()\n",
        "        self.train_xgboost_model()\n",
        "        self.explain_xgboost_model()\n",
        "            \n",
        "            # Generate results\n",
        "        results = self.generate_classification_results()\n",
        "            \n",
        "            # Create visualizations\n",
        "        self.visualize_results()\n",
        "            \n",
        "            # Save everything\n",
        "        self.save_model_and_results()\n",
        "            \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PIPELINE EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*60)\n",
        "            \n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'WellClassificationPipeline' object has no attribute 'run_complete_pipeline'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Initialize and run the pipeline\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m WellClassificationPipeline()\n\u001b[1;32m----> 5\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_complete_pipeline\u001b[49m()\n\u001b[0;32m      7\u001b[0m     reservoir_totals \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mcalculate_reservoir_production()\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(reservoir_totals)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'WellClassificationPipeline' object has no attribute 'run_complete_pipeline'"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize and run the pipeline\n",
        "    pipeline = WellClassificationPipeline()\n",
        "    results = pipeline.run_complete_pipeline()\n",
        "\n",
        "    reservoir_totals = pipeline.calculate_reservoir_production()\n",
        "    print(reservoir_totals)\n",
        "    \n",
        "    print(\"\\nFinal Classification Results:\")\n",
        "    print(results.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNVI5ondeIT8HNAmLTDmk1U",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
