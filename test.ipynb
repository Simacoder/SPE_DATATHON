{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26bfc56",
   "metadata": {},
   "source": [
    "# Testin and working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd172282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPE DSEATS AFRICA 2025 - WELL CLASSIFICATION PIPELINE\n",
      "============================================================\n",
      "Loading datasets...\n",
      "Wells data shape: (7955, 13)\n",
      "Reservoir info shape: (5, 6)\n",
      "Classification params shape: (5, 7)\n",
      "\n",
      "Wells data columns:\n",
      "['PROD_DATE', 'WELL_NAME', 'ON_STREAM_HRS', 'BOTTOMHOLE_FLOWING_PRESSURE (PSI)', 'DOWNHOLE_TEMPERATURE (deg F)', 'ANNULUS_PRESS (PSI)', 'CHOKE_SIZE (%)', 'WELL_HEAD_PRESSURE (PSI)', 'WELL_HEAD_TEMPERATURE (deg F)', 'CUMULATIVE_OIL_PROD (STB)', 'CUMULATIVE_FORMATION_GAS_PROD (MSCF)', 'CUMULATIVE_TOTAL_GAS_PROD (MSCF)', 'CUMULATIVE_WATER_PROD (BBL)']\n",
      "\n",
      "Unique wells: 20\n",
      "Date range: 01-Apr-11 to 31-Oct-15\n",
      "\n",
      "Cleaning data...\n",
      "Data cleaning completed!\n",
      "Final dataset shape: (7955, 13)\n",
      "\n",
      "Engineering features...\n",
      "Feature engineering completed! Shape: (20, 33)\n",
      "Features created: 33\n",
      "\n",
      "Preparing target variables...\n",
      "Target variables prepared!\n",
      "\n",
      "Preparing features for ML...\n",
      "Features prepared! Shape: (20, 31)\n",
      "\n",
      "Training Gaussian Naive Bayes model...\n",
      "Reservoir Name: 0.850\n",
      "Reservoir Type: 1.000\n",
      "Well Type: 1.000\n",
      "Production Type: 1.000\n",
      "Formation GOR Trend: 0.800\n",
      "Watercut Trend: 1.000\n",
      "Oil Productivity Index Trend: 1.000\n",
      "\n",
      "Overall accuracy: 0.950\n",
      "\n",
      "==================================================\n",
      "GAUSSIAN NAIVE BAYES MODEL EXPLANATION\n",
      "==================================================\n",
      "\n",
      "        GAUSSIAN NAIVE BAYES OVERVIEW:\n",
      "        ============================\n",
      "        \n",
      "        Gaussian Naive Bayes is a probabilistic classifier based on Bayes' theorem\n",
      "        with the assumption of independence between features and that continuous\n",
      "        features follow a normal (Gaussian) distribution.\n",
      "        \n",
      "        MATHEMATICAL FOUNDATION:\n",
      "        ======================\n",
      "        \n",
      "        Bayes' Theorem: P(class|features) = P(features|class) * P(class) / P(features)\n",
      "        \n",
      "        For Gaussian NB:\n",
      "        - P(class): Prior probability of each class\n",
      "        - P(features|class): Likelihood assuming Gaussian distribution\n",
      "        - P(xi|class) = (1/√(2π*σ²)) * exp(-(xi-μ)²/(2σ²))\n",
      "        \n",
      "        Where μ is the mean and σ is the standard deviation of feature xi for the given class.\n",
      "        \n",
      "        KEY ASSUMPTIONS:\n",
      "        ==============\n",
      "        1. Feature Independence: Features are conditionally independent given the class\n",
      "        2. Gaussian Distribution: Continuous features follow normal distribution\n",
      "        3. No missing values in prediction phase\n",
      "        \n",
      "        MODEL PARAMETERS:\n",
      "        ===============\n",
      "        \n",
      "\n",
      "Reservoir Name:\n",
      "  - Classes: ['ACHI' 'DEPU' 'KEMA' 'MAKO']\n",
      "  - Class prior probabilities: [0.4 0.3 0.1 0.2]\n",
      "  - Feature means shape: (4, 31)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GaussianNB' object has no attribute 'sigma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 841\u001b[0m\n\u001b[0;32m    838\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m WellClassificationPipeline()\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# Run complete pipeline\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;66;03m# Perform cross-validation\u001b[39;00m\n\u001b[0;32m    844\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mcross_validate_model()\n",
      "Cell \u001b[1;32mIn[18], line 692\u001b[0m, in \u001b[0;36mWellClassificationPipeline.run_complete_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_gaussian_nb_model()\n\u001b[0;32m    691\u001b[0m \u001b[38;5;66;03m# Step 7: Explain model\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_gaussian_nb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;66;03m# Step 8: Generate results\u001b[39;00m\n\u001b[0;32m    695\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_classification_results()\n",
      "Cell \u001b[1;32mIn[18], line 497\u001b[0m, in \u001b[0;36mWellClassificationPipeline.explain_gaussian_nb\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Class prior probabilities: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mclass_prior_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Feature means shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mtheta_\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Feature variances shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma_\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;124m\u001b[39m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;124mADVANTAGES FOR THIS PROBLEM:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124mare more discriminative for classification.\u001b[39m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GaussianNB' object has no attribute 'sigma_'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SPE DSEATS Africa Datathon 2025 - Well Classification Pipeline\n",
    "Complete pipeline from data cleaning to Gaussian Naive Bayes classification\n",
    "\"\"\"\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class WellClassificationPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for well classification using Gaussian Naive Bayes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wells_data = None\n",
    "        self.reservoir_info = None\n",
    "        self.classification_params = None\n",
    "        self.processed_features = None\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.target_columns = [\n",
    "            'Reservoir Name', 'Reservoir Type', 'Well Type', 'Production Type',\n",
    "            'Formation GOR Trend', 'Watercut Trend', 'Oil Productivity Index Trend'\n",
    "        ]\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and examine the datasets\"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "        \n",
    "        # Load wells production data\n",
    "        self.wells_data = pd.read_csv('data/spe_africa_dseats_datathon_2025_wells_dataset.csv')\n",
    "        \n",
    "        # Load reservoir information\n",
    "        self.reservoir_info = pd.read_csv('data/reservoir_info.csv')\n",
    "        \n",
    "        # Load classification parameters (target classes)\n",
    "        self.classification_params = pd.read_csv('data/classification_parameters.csv')\n",
    "        \n",
    "        print(f\"Wells data shape: {self.wells_data.shape}\")\n",
    "        print(f\"Reservoir info shape: {self.reservoir_info.shape}\")\n",
    "        print(f\"Classification params shape: {self.classification_params.shape}\")\n",
    "        \n",
    "        # Display basic info\n",
    "        print(\"\\nWells data columns:\")\n",
    "        print(self.wells_data.columns.tolist())\n",
    "        print(f\"\\nUnique wells: {self.wells_data['WELL_NAME'].nunique()}\")\n",
    "        print(f\"Date range: {self.wells_data['PROD_DATE'].min()} to {self.wells_data['PROD_DATE'].max()}\")\n",
    "        \n",
    "    def clean_data(self):\n",
    "        \"\"\"Clean and preprocess the raw data\"\"\"\n",
    "        print(\"\\nCleaning data...\")\n",
    "        \n",
    "        # Convert date column\n",
    "        self.wells_data['PROD_DATE'] = pd.to_datetime(self.wells_data['PROD_DATE'])\n",
    "        \n",
    "        # Clean numeric columns that might have commas\n",
    "        numeric_columns = [\n",
    "            'BOTTOMHOLE_FLOWING_PRESSURE (PSI)',\n",
    "            'ANNULUS_PRESS (PSI)', \n",
    "            'WELL_HEAD_PRESSURE (PSI)',\n",
    "            'CUMULATIVE_OIL_PROD (STB)',\n",
    "            'CUMULATIVE_FORMATION_GAS_PROD (MSCF)',\n",
    "            'CUMULATIVE_TOTAL_GAS_PROD (MSCF)',\n",
    "            'CUMULATIVE_WATER_PROD (BBL)'\n",
    "        ]\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.wells_data.columns:\n",
    "                # Remove commas and convert to numeric\n",
    "                self.wells_data[col] = pd.to_numeric(\n",
    "                    self.wells_data[col].astype(str).str.replace(',', ''),\n",
    "                    errors='coerce'\n",
    "                )\n",
    "        \n",
    "        # Handle missing values\n",
    "        self.wells_data = self.wells_data.fillna(0)\n",
    "        \n",
    "        # Sort by well name and date\n",
    "        self.wells_data = self.wells_data.sort_values(['WELL_NAME', 'PROD_DATE'])\n",
    "        \n",
    "        print(\"Data cleaning completed!\")\n",
    "        print(f\"Final dataset shape: {self.wells_data.shape}\")\n",
    "        \n",
    "    def feature_engineering(self):\n",
    "        \"\"\"Engineer features from time series production data\"\"\"\n",
    "        print(\"\\nEngineering features...\")\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for well in self.wells_data['WELL_NAME'].unique():\n",
    "            well_data = self.wells_data[self.wells_data['WELL_NAME'] == well].copy()\n",
    "            well_data = well_data.sort_values('PROD_DATE')\n",
    "            \n",
    "            # Calculate daily production rates\n",
    "            well_data['Daily_Oil_Prod'] = well_data['CUMULATIVE_OIL_PROD (STB)'].diff().fillna(0)\n",
    "            well_data['Daily_Gas_Prod'] = well_data['CUMULATIVE_FORMATION_GAS_PROD (MSCF)'].diff().fillna(0)\n",
    "            well_data['Daily_Water_Prod'] = well_data['CUMULATIVE_WATER_PROD (BBL)'].diff().fillna(0)\n",
    "            \n",
    "            # Calculate GOR (Gas Oil Ratio)\n",
    "            well_data['GOR'] = np.where(\n",
    "                well_data['Daily_Oil_Prod'] > 0,\n",
    "                well_data['Daily_Gas_Prod'] / well_data['Daily_Oil_Prod'] * 1000,  # Convert to SCF/STB\n",
    "                0\n",
    "            )\n",
    "            \n",
    "            # Calculate water cut\n",
    "            total_liquid = well_data['Daily_Oil_Prod'] + well_data['Daily_Water_Prod']\n",
    "            well_data['Water_Cut'] = np.where(\n",
    "                total_liquid > 0,\n",
    "                well_data['Daily_Water_Prod'] / total_liquid * 100,\n",
    "                0\n",
    "            )\n",
    "            \n",
    "            # Calculate productivity index (approximate)\n",
    "            # PI = Oil Rate / (Static Pressure - BHP)\n",
    "            # Assuming average reservoir pressure as static pressure\n",
    "            avg_reservoir_pressure = 3500  # We'll refine this later\n",
    "            well_data['Productivity_Index'] = np.where(\n",
    "                well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'] > 0,\n",
    "                well_data['Daily_Oil_Prod'] / (avg_reservoir_pressure - well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)']),\n",
    "                0\n",
    "            )\n",
    "            \n",
    "            # Aggregate features for the well\n",
    "            well_features = {\n",
    "                'WELL_NAME': well,\n",
    "                'Well_Number': int(well.split('_#')[1]),\n",
    "                \n",
    "                # Production statistics\n",
    "                'Total_Oil_Prod': well_data['CUMULATIVE_OIL_PROD (STB)'].max(),\n",
    "                'Total_Gas_Prod': well_data['CUMULATIVE_FORMATION_GAS_PROD (MSCF)'].max(),\n",
    "                'Total_Water_Prod': well_data['CUMULATIVE_WATER_PROD (BBL)'].max(),\n",
    "                'Avg_Daily_Oil': well_data['Daily_Oil_Prod'].mean(),\n",
    "                'Max_Daily_Oil': well_data['Daily_Oil_Prod'].max(),\n",
    "                'Oil_Production_Decline': self._calculate_decline_rate(well_data['Daily_Oil_Prod']),\n",
    "                \n",
    "                # Pressure statistics\n",
    "                'Avg_BHP': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].mean(),\n",
    "                'Max_BHP': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].max(),\n",
    "                'Min_BHP': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].min(),\n",
    "                'BHP_Variance': well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].var(),\n",
    "                'Avg_WHP': well_data['WELL_HEAD_PRESSURE (PSI)'].mean(),\n",
    "                'Avg_Annulus_Press': well_data['ANNULUS_PRESS (PSI)'].mean(),\n",
    "                \n",
    "                # Temperature statistics\n",
    "                'Avg_Downhole_Temp': well_data['DOWNHOLE_TEMPERATURE (deg F)'].mean(),\n",
    "                'Avg_Wellhead_Temp': well_data['WELL_HEAD_TEMPERATURE (deg F)'].mean(),\n",
    "                \n",
    "                # Flow characteristics\n",
    "                'Avg_Choke_Size': well_data['CHOKE_SIZE (%)'].mean(),\n",
    "                'Choke_Variance': well_data['CHOKE_SIZE (%)'].var(),\n",
    "                'Avg_Onstream_Hours': well_data['ON_STREAM_HRS'].mean(),\n",
    "                \n",
    "                # GOR and Water Cut trends\n",
    "                'Avg_GOR': well_data['GOR'].mean(),\n",
    "                'GOR_Trend': self._calculate_trend(well_data['GOR']),\n",
    "                'Max_GOR': well_data['GOR'].max(),\n",
    "                'Avg_Water_Cut': well_data['Water_Cut'].mean(),\n",
    "                'Water_Cut_Trend': self._calculate_trend(well_data['Water_Cut']),\n",
    "                'Max_Water_Cut': well_data['Water_Cut'].max(),\n",
    "                \n",
    "                # Productivity Index\n",
    "                'Avg_PI': well_data['Productivity_Index'].mean(),\n",
    "                'PI_Trend': self._calculate_trend(well_data['Productivity_Index']),\n",
    "                'Max_PI': well_data['Productivity_Index'].max(),\n",
    "                \n",
    "                # Production stability\n",
    "                'Production_Stability': self._calculate_stability(well_data['Daily_Oil_Prod']),\n",
    "                'Days_Produced': len(well_data[well_data['Daily_Oil_Prod'] > 0]),\n",
    "                'Production_Efficiency': well_data['ON_STREAM_HRS'].mean() / 24.0,\n",
    "                \n",
    "                # Well type indicators\n",
    "                'Has_Annulus_Pressure': (well_data['ANNULUS_PRESS (PSI)'] > 0).any(),\n",
    "                'Pressure_Differential': well_data['WELL_HEAD_PRESSURE (PSI)'].mean() - well_data['BOTTOMHOLE_FLOWING_PRESSURE (PSI)'].mean(),\n",
    "            }\n",
    "            \n",
    "            features_list.append(well_features)\n",
    "        \n",
    "        self.processed_features = pd.DataFrame(features_list)\n",
    "        print(f\"Feature engineering completed! Shape: {self.processed_features.shape}\")\n",
    "        print(f\"Features created: {len(self.processed_features.columns)}\")\n",
    "        \n",
    "    def _calculate_decline_rate(self, production_series):\n",
    "        \"\"\"Calculate production decline rate\"\"\"\n",
    "        if len(production_series) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # Use linear regression to estimate decline\n",
    "        x = np.arange(len(production_series))\n",
    "        y = production_series.values\n",
    "        \n",
    "        # Remove zeros for log calculation\n",
    "        y_nonzero = y[y > 0]\n",
    "        if len(y_nonzero) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # Simple linear decline\n",
    "        if len(y_nonzero) == len(y):\n",
    "            slope = np.polyfit(x, y, 1)[0]\n",
    "            return slope / np.mean(y) if np.mean(y) > 0 else 0\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def _calculate_trend(self, series):\n",
    "        \"\"\"Calculate trend direction: 1 for increasing, -1 for decreasing, 0 for flat\"\"\"\n",
    "        if len(series) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # Remove outliers using IQR\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filtered_series = series[(series >= Q1 - 1.5*IQR) & (series <= Q3 + 1.5*IQR)]\n",
    "        \n",
    "        if len(filtered_series) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate trend using linear regression\n",
    "        x = np.arange(len(filtered_series))\n",
    "        slope = np.polyfit(x, filtered_series.values, 1)[0]\n",
    "        \n",
    "        # Threshold for significance\n",
    "        threshold = 0.01 * np.mean(filtered_series)\n",
    "        \n",
    "        if slope > threshold:\n",
    "            return 1  # Increasing\n",
    "        elif slope < -threshold:\n",
    "            return -1  # Decreasing\n",
    "        else:\n",
    "            return 0  # Flat\n",
    "    \n",
    "    def _calculate_stability(self, production_series):\n",
    "        \"\"\"Calculate production stability (coefficient of variation)\"\"\"\n",
    "        if len(production_series) < 2 or production_series.mean() == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate coefficient of variation\n",
    "        cv = production_series.std() / production_series.mean()\n",
    "        return 1 / (1 + cv)  # Normalize to 0-1 where 1 is most stable\n",
    "    \n",
    "    def prepare_target_variables(self):\n",
    "        \"\"\"Prepare target variables for classification\"\"\"\n",
    "        print(\"\\nPreparing target variables...\")\n",
    "        \n",
    "        # Create a mapping from well number to classifications\n",
    "        # This would normally come from domain knowledge or labeled data\n",
    "        # For now, we'll use the classification parameters as a reference\n",
    "        \n",
    "        # Initialize target dataframe\n",
    "        target_df = pd.DataFrame({\n",
    "            'Well_Number': range(1, 21),\n",
    "            'Reservoir Name': None,\n",
    "            'Reservoir Type': None,\n",
    "            'Well Type': None,\n",
    "            'Production Type': None,\n",
    "            'Formation GOR Trend': None,\n",
    "            'Watercut Trend': None,\n",
    "            'Oil Productivity Index Trend': None\n",
    "        })\n",
    "        \n",
    "        # This is where you would implement the classification logic\n",
    "        # based on the reservoir engineering rules provided in the challenge\n",
    "        \n",
    "        # For demonstration, let's implement some basic rules:\n",
    "        for idx, row in self.processed_features.iterrows():\n",
    "            well_num = row['Well_Number']\n",
    "            \n",
    "            # Reservoir identification based on BHP and reservoir pressures\n",
    "            reservoir_name = self._identify_reservoir(row)\n",
    "            target_df.loc[well_num-1, 'Reservoir Name'] = reservoir_name\n",
    "            \n",
    "            # Reservoir type based on reservoir info\n",
    "            reservoir_type = self._determine_reservoir_type(reservoir_name)\n",
    "            target_df.loc[well_num-1, 'Reservoir Type'] = reservoir_type\n",
    "            \n",
    "            # Well type based on annulus pressure\n",
    "            well_type = 'GL' if row['Has_Annulus_Pressure'] else 'NF'\n",
    "            target_df.loc[well_num-1, 'Well Type'] = well_type\n",
    "            \n",
    "            # Production type based on stability\n",
    "            prod_type = 'Steady' if row['Production_Stability'] > 0.5 else 'Unsteady'\n",
    "            target_df.loc[well_num-1, 'Production Type'] = prod_type\n",
    "            \n",
    "            # GOR trend classification\n",
    "            gor_trend = self._classify_gor_trend(row, reservoir_name)\n",
    "            target_df.loc[well_num-1, 'Formation GOR Trend'] = gor_trend\n",
    "            \n",
    "            # Water cut trend\n",
    "            wc_trend = self._classify_watercut_trend(row['Water_Cut_Trend'])\n",
    "            target_df.loc[well_num-1, 'Watercut Trend'] = wc_trend\n",
    "            \n",
    "            # PI trend\n",
    "            pi_trend = self._classify_pi_trend(row['PI_Trend'])\n",
    "            target_df.loc[well_num-1, 'Oil Productivity Index Trend'] = pi_trend\n",
    "        \n",
    "        self.target_data = target_df\n",
    "        print(\"Target variables prepared!\")\n",
    "        \n",
    "    def _identify_reservoir(self, well_features):\n",
    "        \"\"\"Identify reservoir based on pressure characteristics\"\"\"\n",
    "        bhp = well_features['Avg_BHP']\n",
    "        \n",
    "        # Reservoir pressure ranges (with 200 psi differential allowance)\n",
    "        reservoir_pressures = {\n",
    "            'ACHI': 2700,\n",
    "            'KEMA': 3900,\n",
    "            'MAKO': 3000,\n",
    "            'DEPU': 2400,\n",
    "            'JANI': 4200\n",
    "        }\n",
    "        \n",
    "        # Find closest match within 200 psi\n",
    "        min_diff = float('inf')\n",
    "        best_reservoir = 'ACHI'\n",
    "        \n",
    "        for reservoir, pressure in reservoir_pressures.items():\n",
    "            diff = abs(bhp - pressure)\n",
    "            if diff < min_diff and diff <= 200:\n",
    "                min_diff = diff\n",
    "                best_reservoir = reservoir\n",
    "        \n",
    "        return best_reservoir\n",
    "    \n",
    "    def _determine_reservoir_type(self, reservoir_name):\n",
    "        \"\"\"Determine if reservoir is saturated or undersaturated\"\"\"\n",
    "        # Based on reservoir info provided\n",
    "        reservoir_types = {\n",
    "            'ACHI': 'Saturated',\n",
    "            'KEMA': 'Undersat',\n",
    "            'MAKO': 'Saturated',  # Assuming based on bubble point = initial pressure\n",
    "            'DEPU': 'Saturated',\n",
    "            'JANI': 'Undersat'\n",
    "        }\n",
    "        return reservoir_types.get(reservoir_name, 'Saturated')\n",
    "    \n",
    "    def _classify_gor_trend(self, well_features, reservoir_name):\n",
    "        \"\"\"Classify GOR trend relative to solution GOR\"\"\"\n",
    "        # Solution GOR from reservoir info\n",
    "        solution_gor = {\n",
    "            'ACHI': 800,\n",
    "            'KEMA': 600,\n",
    "            'MAKO': 500,\n",
    "            'DEPU': 1200,\n",
    "            'JANI': 1000\n",
    "        }\n",
    "        \n",
    "        avg_gor = well_features['Avg_GOR']\n",
    "        sol_gor = solution_gor.get(reservoir_name, 800)\n",
    "        \n",
    "        if avg_gor > sol_gor * 1.1:\n",
    "            return 'aSolGOR'\n",
    "        elif avg_gor < sol_gor * 0.9:\n",
    "            return 'bSolGOR'\n",
    "        else:\n",
    "            return 'Combo'\n",
    "    \n",
    "    def _classify_watercut_trend(self, trend_value):\n",
    "        \"\"\"Classify water cut trend\"\"\"\n",
    "        if trend_value > 0.5:\n",
    "            return 'Incr'\n",
    "        elif trend_value < -0.5:\n",
    "            return 'Decr'\n",
    "        else:\n",
    "            return 'Flat'\n",
    "    \n",
    "    def _classify_pi_trend(self, trend_value):\n",
    "        \"\"\"Classify productivity index trend\"\"\"\n",
    "        if trend_value > 0.5:\n",
    "            return 'Incr'\n",
    "        elif trend_value < -0.5:\n",
    "            return 'Decr'\n",
    "        else:\n",
    "            return 'Flat'\n",
    "    \n",
    "    def prepare_features_for_ml(self):\n",
    "        \"\"\"Prepare features for machine learning\"\"\"\n",
    "        print(\"\\nPreparing features for ML...\")\n",
    "        \n",
    "        # Select numerical features for ML\n",
    "        feature_columns = [\n",
    "            'Total_Oil_Prod', 'Total_Gas_Prod', 'Total_Water_Prod',\n",
    "            'Avg_Daily_Oil', 'Max_Daily_Oil', 'Oil_Production_Decline',\n",
    "            'Avg_BHP', 'Max_BHP', 'Min_BHP', 'BHP_Variance',\n",
    "            'Avg_WHP', 'Avg_Annulus_Press',\n",
    "            'Avg_Downhole_Temp', 'Avg_Wellhead_Temp',\n",
    "            'Avg_Choke_Size', 'Choke_Variance', 'Avg_Onstream_Hours',\n",
    "            'Avg_GOR', 'GOR_Trend', 'Max_GOR',\n",
    "            'Avg_Water_Cut', 'Water_Cut_Trend', 'Max_Water_Cut',\n",
    "            'Avg_PI', 'PI_Trend', 'Max_PI',\n",
    "            'Production_Stability', 'Days_Produced', 'Production_Efficiency',\n",
    "            'Pressure_Differential'\n",
    "        ]\n",
    "        \n",
    "        # Convert boolean to numeric\n",
    "        self.processed_features['Has_Annulus_Pressure'] = self.processed_features['Has_Annulus_Pressure'].astype(int)\n",
    "        feature_columns.append('Has_Annulus_Pressure')\n",
    "        \n",
    "        # Create feature matrix\n",
    "        X = self.processed_features[feature_columns].fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.X_scaled = pd.DataFrame(X_scaled, columns=feature_columns)\n",
    "        \n",
    "        print(f\"Features prepared! Shape: {self.X_scaled.shape}\")\n",
    "        \n",
    "    def train_gaussian_nb_model(self):\n",
    "        \"\"\"Train Gaussian Naive Bayes model\"\"\"\n",
    "        print(\"\\nTraining Gaussian Naive Bayes model...\")\n",
    "        \n",
    "        # Prepare target variables\n",
    "        y_encoded = {}\n",
    "        \n",
    "        for target_col in self.target_columns:\n",
    "            le = LabelEncoder()\n",
    "            y_encoded[target_col] = le.fit_transform(self.target_data[target_col].astype(str))\n",
    "            self.label_encoders[target_col] = le\n",
    "        \n",
    "        # Create multi-output classifier\n",
    "        self.model = MultiOutputClassifier(GaussianNB())\n",
    "        \n",
    "        # Prepare target matrix\n",
    "        y_matrix = np.column_stack([y_encoded[col] for col in self.target_columns])\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(self.X_scaled, y_matrix)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(self.X_scaled)\n",
    "        \n",
    "        # Calculate accuracy for each target\n",
    "        accuracies = {}\n",
    "        for i, target_col in enumerate(self.target_columns):\n",
    "            acc = accuracy_score(y_matrix[:, i], y_pred[:, i])\n",
    "            accuracies[target_col] = acc\n",
    "            print(f\"{target_col}: {acc:.3f}\")\n",
    "        \n",
    "        print(f\"\\nOverall accuracy: {np.mean(list(accuracies.values())):.3f}\")\n",
    "        \n",
    "    def explain_gaussian_nb(self):\n",
    "        \"\"\"Explain Gaussian Naive Bayes model and parameters\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"GAUSSIAN NAIVE BAYES MODEL EXPLANATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        GAUSSIAN NAIVE BAYES OVERVIEW:\n",
    "        ============================\n",
    "        \n",
    "        Gaussian Naive Bayes is a probabilistic classifier based on Bayes' theorem\n",
    "        with the assumption of independence between features and that continuous\n",
    "        features follow a normal (Gaussian) distribution.\n",
    "        \n",
    "        MATHEMATICAL FOUNDATION:\n",
    "        ======================\n",
    "        \n",
    "        Bayes' Theorem: P(class|features) = P(features|class) * P(class) / P(features)\n",
    "        \n",
    "        For Gaussian NB:\n",
    "        - P(class): Prior probability of each class\n",
    "        - P(features|class): Likelihood assuming Gaussian distribution\n",
    "        - P(xi|class) = (1/√(2π*σ²)) * exp(-(xi-μ)²/(2σ²))\n",
    "        \n",
    "        Where μ is the mean and σ is the standard deviation of feature xi for the given class.\n",
    "        \n",
    "        KEY ASSUMPTIONS:\n",
    "        ==============\n",
    "        1. Feature Independence: Features are conditionally independent given the class\n",
    "        2. Gaussian Distribution: Continuous features follow normal distribution\n",
    "        3. No missing values in prediction phase\n",
    "        \n",
    "        MODEL PARAMETERS:\n",
    "        ===============\n",
    "        \"\"\")\n",
    "        \n",
    "        # Display model parameters for each target\n",
    "        for i, target_col in enumerate(self.target_columns):\n",
    "            estimator = self.model.estimators_[i]\n",
    "         \n",
    "            print(f\"\\n{target_col}:\")\n",
    "            print(f\"  - Classes: {self.label_encoders[target_col].classes_}\")\n",
    "            print(f\"  - Class prior probabilities: {estimator.class_prior_}\")\n",
    "            print(f\"  - Feature means shape: {estimator.theta_.shape}\")\n",
    "            print(f\" - Feature variances shape: {estimator.var_.shape}\")\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        \n",
    "        ADVANTAGES FOR THIS PROBLEM:\n",
    "        ==========================\n",
    "        1. Works well with small datasets (20 wells)\n",
    "        2. Fast training and prediction\n",
    "        3. Handles multiple classes naturally\n",
    "        4. Provides probabilistic outputs\n",
    "        5. Robust to irrelevant features\n",
    "        6. No hyperparameter tuning required\n",
    "        \n",
    "        LIMITATIONS:\n",
    "        ==========\n",
    "        1. Strong independence assumption\n",
    "        2. Assumes Gaussian distribution for continuous features\n",
    "        3. Can be affected by skewed distributions\n",
    "        4. May not capture complex feature interactions\n",
    "        \n",
    "        FEATURE IMPORTANCE:\n",
    "        =================\n",
    "        Features with higher variance differences between classes\n",
    "        are more discriminative for classification.\n",
    "        \"\"\")\n",
    "        \n",
    "    def generate_classification_results(self):\n",
    "        \"\"\"Generate final classification results\"\"\"\n",
    "        print(\"\\nGenerating classification results...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(self.X_scaled)\n",
    "        \n",
    "        # Convert predictions back to original labels\n",
    "        results = []\n",
    "        for i in range(len(y_pred)):\n",
    "            well_result = {'Well': i + 1}\n",
    "            for j, target_col in enumerate(self.target_columns):\n",
    "                predicted_class = self.label_encoders[target_col].inverse_transform([y_pred[i][j]])[0]\n",
    "                well_result[target_col] = predicted_class\n",
    "            results.append(well_result)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save results\n",
    "        results_df.to_csv('Team_DSEATS_Africa_2025_Classification.csv', index=False)\n",
    "        \n",
    "        print(\"Classification results generated and saved!\")\n",
    "        print(results_df.head())\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create visualizations for EDA and results\"\"\"\n",
    "        print(\"\\nCreating visualizations...\")\n",
    "        \n",
    "        # Set up the plotting\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Plot 1: Production Profile by Well\n",
    "        ax1 = axes[0, 0]\n",
    "        for well in self.processed_features['Well_Number'].head(5):\n",
    "            well_data = self.wells_data[self.wells_data['WELL_NAME'] == f'WELL_#{well}']\n",
    "            ax1.plot(well_data['PROD_DATE'], well_data['CUMULATIVE_OIL_PROD (STB)'], \n",
    "                    label=f'Well #{well}', linewidth=2)\n",
    "        ax1.set_title('Cumulative Oil Production by Well', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Cumulative Oil (STB)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Pressure Distribution\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.boxplot([self.processed_features['Avg_BHP'], \n",
    "                    self.processed_features['Avg_WHP']], \n",
    "                   labels=['BHP', 'WHP'])\n",
    "        ax2.set_title('Pressure Distribution', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('Pressure (PSI)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: GOR vs Water Cut\n",
    "        ax3 = axes[0, 2]\n",
    "        scatter = ax3.scatter(self.processed_features['Avg_GOR'], \n",
    "                             self.processed_features['Avg_Water_Cut'],\n",
    "                             c=self.processed_features['Well_Number'], \n",
    "                             cmap='viridis', s=100, alpha=0.7)\n",
    "        ax3.set_title('GOR vs Water Cut', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Average GOR (SCF/STB)')\n",
    "        ax3.set_ylabel('Average Water Cut (%)')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax3, label='Well Number')\n",
    "        \n",
    "        # Plot 4: Production Stability\n",
    "        ax4 = axes[1, 0]\n",
    "        ax4.hist(self.processed_features['Production_Stability'], \n",
    "                bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax4.set_title('Production Stability Distribution', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Production Stability Index')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Reservoir Classification\n",
    "        ax5 = axes[1, 1]\n",
    "        reservoir_counts = self.target_data['Reservoir Name'].value_counts()\n",
    "        ax5.pie(reservoir_counts.values, labels=reservoir_counts.index, \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "        ax5.set_title('Reservoir Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 6: Feature Correlation Heatmap\n",
    "        ax6 = axes[1, 2]\n",
    "        important_features = ['Avg_BHP', 'Avg_GOR', 'Avg_Water_Cut', 'Production_Stability', \n",
    "                             'Total_Oil_Prod', 'Avg_PI']\n",
    "        corr_matrix = self.processed_features[important_features].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax6)\n",
    "        ax6.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('well_classification_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional plot: Model Performance\n",
    "        self._plot_model_performance()\n",
    "        \n",
    "    def _plot_model_performance(self):\n",
    "        \"\"\"Plot model performance metrics\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot 1: Accuracy by Target Variable\n",
    "        ax1 = axes[0]\n",
    "        y_pred = self.model.predict(self.X_scaled)\n",
    "        accuracies = []\n",
    "        \n",
    "        for i, target_col in enumerate(self.target_columns):\n",
    "            y_true = self.target_data[target_col].astype(str)\n",
    "            y_pred_decoded = self.label_encoders[target_col].inverse_transform(y_pred[:, i])\n",
    "            acc = accuracy_score(y_true, y_pred_decoded)\n",
    "            accuracies.append(acc)\n",
    "        \n",
    "        bars = ax1.bar(range(len(self.target_columns)), accuracies, \n",
    "                      color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8'])\n",
    "        ax1.set_title('Model Accuracy by Target Variable', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Target Variables')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_xticks(range(len(self.target_columns)))\n",
    "        ax1.set_xticklabels([col.replace(' ', '\\n') for col in self.target_columns], rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "    \n",
    "        # Plot 2: Prediction Confidence\n",
    "        ax2 = axes[1]\n",
    "        # Get prediction probabilities for first target (Reservoir Name)\n",
    "        proba = self.model.estimators_[0].predict_proba(self.X_scaled)\n",
    "        max_proba = np.max(proba, axis=1)\n",
    "\n",
    "        ax2.hist(max_proba, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        ax2.set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Maximum Prediction Probability')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_performance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Run the complete pipeline from data loading to results generation\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"SPE DSEATS AFRICA 2025 - WELL CLASSIFICATION PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "            \n",
    "        # Step 1: Load data\n",
    "        self.load_data()\n",
    "            \n",
    "        # Step 2: Clean data\n",
    "        self.clean_data()\n",
    "            \n",
    "        # Step 3: Feature engineering\n",
    "        self.feature_engineering()\n",
    "            \n",
    "        # Step 4: Prepare target variables\n",
    "        self.prepare_target_variables()\n",
    "            \n",
    "        # Step 5: Prepare features for ML\n",
    "        self.prepare_features_for_ml()\n",
    "            \n",
    "        # Step 6: Train model\n",
    "        self.train_gaussian_nb_model()\n",
    "            \n",
    "        # Step 7: Explain model\n",
    "        self.explain_gaussian_nb()\n",
    "            \n",
    "        # Step 8: Generate results\n",
    "        results = self.generate_classification_results()\n",
    "            \n",
    "        # Step 9: Create visualizations\n",
    "        self.visualize_results()\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def cross_validate_model(self):\n",
    "        \"\"\"Perform cross-validation to assess model robustness\"\"\"\n",
    "        print(\"\\nPerforming cross-validation...\")\n",
    "            \n",
    "        cv_scores = {}\n",
    "            \n",
    "        for i, target_col in enumerate(self.target_columns):\n",
    "            # Get target values\n",
    "            y_true = self.target_data[target_col].astype(str)\n",
    "            y_encoded = self.label_encoders[target_col].transform(y_true)\n",
    "                \n",
    "            # Single-output classifier for cross-validation\n",
    "            single_model = GaussianNB()\n",
    "                \n",
    "            # Perform 5-fold cross-validation\n",
    "            scores = cross_val_score(single_model, self.X_scaled, y_encoded, \n",
    "                                cv=5, scoring='accuracy')\n",
    "                \n",
    "            cv_scores[target_col] = {\n",
    "                 'mean': scores.mean(),\n",
    "                 'std': scores.std(),\n",
    "                 'scores': scores\n",
    "              }\n",
    "               \n",
    "            print(f\"{target_col}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "            \n",
    "        return cv_scores\n",
    "        \n",
    "    def generate_detailed_report(self):\n",
    "        \"\"\"Generate a detailed classification report\"\"\"\n",
    "        print(\"\\nGenerating detailed classification report...\")\n",
    "            \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(self.X_scaled)\n",
    "            \n",
    "        # Generate report for each target\n",
    "        reports = {}\n",
    "            \n",
    "        for i, target_col in enumerate(self.target_columns):\n",
    "            y_true = self.target_data[target_col].astype(str)\n",
    "            y_pred_decoded = self.label_encoders[target_col].inverse_transform(y_pred[:, i])\n",
    "                \n",
    "            # Classification report\n",
    "            report = classification_report(y_true, y_pred_decoded, output_dict=True)\n",
    "            reports[target_col] = report\n",
    "                \n",
    "            print(f\"\\n{target_col}:\")\n",
    "            print(classification_report(y_true, y_pred_decoded))\n",
    "                \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred_decoded)\n",
    "            print(f\"Confusion Matrix:\\n{cm}\")\n",
    "            \n",
    "        return reports\n",
    "        \n",
    "    def save_model_artifacts(self):\n",
    "         \"\"\"Save model and preprocessing artifacts\"\"\"\n",
    "\n",
    "        \n",
    "            \n",
    "            # Save model\n",
    "         with open('gaussian_nb_model.pkl', 'wb') as f:\n",
    "                pickle.dump(self.model, f)\n",
    "                \n",
    "            # Save scaler\n",
    "         with open('feature_scaler.pkl', 'wb') as f:\n",
    "                pickle.dump(self.scaler, f)\n",
    "                \n",
    "            # Save label encoders\n",
    "         with open('label_encoders.pkl', 'wb') as f:\n",
    "                pickle.dump(self.label_encoders, f)\n",
    "                \n",
    "            # Save processed features\n",
    "         self.processed_features.to_csv('processed_features.csv', index=False)\n",
    "                \n",
    "            # Save target data\n",
    "         self.target_data.to_csv('target_data.csv', index=False)\n",
    "                \n",
    "         print(\"Model artifacts saved successfully!\")\n",
    "            \n",
    "    def load_model_artifacts(self):\n",
    "        \"\"\"Load saved model and preprocessing artifacts\"\"\"\n",
    "        \n",
    "            \n",
    "        # Load model\n",
    "        with open('gaussian_nb_model.pkl', 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "            \n",
    "        # Load scaler\n",
    "        with open('feature_scaler.pkl', 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "            \n",
    "        # Load label encoders\n",
    "        with open('label_encoders.pkl', 'rb') as f:\n",
    "            self.label_encoders = pickle.load(f)\n",
    "            \n",
    "        print(\"Model artifacts loaded successfully!\")\n",
    "        \n",
    "    def predict_new_well(self, well_features):\n",
    "        \"\"\"Predict classifications for a new well\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Run train_gaussian_nb_model() first.\")\n",
    "            \n",
    "        # Scale features\n",
    "        well_features_scaled = self.scaler.transform([well_features])\n",
    "            \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(well_features_scaled)[0]\n",
    "            \n",
    "        # Get probabilities\n",
    "        probabilities = {}\n",
    "        for i, target_col in enumerate(self.target_columns):\n",
    "            proba = self.model.estimators_[i].predict_proba(well_features_scaled)[0]\n",
    "            probabilities[target_col] = dict(zip(\n",
    "                self.label_encoders[target_col].classes_,\n",
    "                proba\n",
    "             ))\n",
    "            \n",
    "        # Decode predictions\n",
    "        results = {}\n",
    "        for i, target_col in enumerate(self.target_columns):\n",
    "            predicted_class = self.label_encoders[target_col].inverse_transform([prediction[i]])[0]\n",
    "            results[target_col] = {\n",
    "               'prediction': predicted_class,\n",
    "               'probabilities': probabilities[target_col]\n",
    "            }\n",
    "            \n",
    "        return results\n",
    "\n",
    "    # Example usage and execution\n",
    "if __name__ == \"__main__\":\n",
    "        # Initialize pipeline\n",
    "        pipeline = WellClassificationPipeline()\n",
    "        \n",
    "        # Run complete pipeline\n",
    "        results = pipeline.run_complete_pipeline()\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = pipeline.cross_validate_model()\n",
    "        \n",
    "        # Generate detailed report\n",
    "        detailed_reports = pipeline.generate_detailed_report()\n",
    "        \n",
    "        # Save model artifacts\n",
    "        pipeline.save_model_artifacts()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ADDITIONAL ANALYSIS COMPLETED!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Example of predicting for a new well\n",
    "        print(\"\\nExample prediction for a hypothetical well:\")\n",
    "        example_features = [\n",
    "            50000,  # Total_Oil_Prod\n",
    "            150000,  # Total_Gas_Prod\n",
    "            20000,  # Total_Water_Prod\n",
    "            200,  # Avg_Daily_Oil\n",
    "            500,  # Max_Daily_Oil\n",
    "            -0.1,  # Oil_Production_Decline\n",
    "            2800,  # Avg_BHP\n",
    "            3000,  # Max_BHP\n",
    "            2500,  # Min_BHP\n",
    "            10000,  # BHP_Variance\n",
    "            150,  # Avg_WHP\n",
    "            50,  # Avg_Annulus_Press\n",
    "            180,  # Avg_Downhole_Temp\n",
    "            120,  # Avg_Wellhead_Temp\n",
    "            45,  # Avg_Choke_Size\n",
    "            100,  # Choke_Variance\n",
    "            20,  # Avg_Onstream_Hours\n",
    "            800,  # Avg_GOR\n",
    "            0,  # GOR_Trend\n",
    "            1200,  # Max_GOR\n",
    "            30,  # Avg_Water_Cut\n",
    "            1,  # Water_Cut_Trend\n",
    "            60,  # Max_Water_Cut\n",
    "            0.5,  # Avg_PI\n",
    "            0,  # PI_Trend\n",
    "            1.0,  # Max_PI\n",
    "            0.7,  # Production_Stability\n",
    "            300,  # Days_Produced\n",
    "            0.8,  # Production_Efficiency\n",
    "            2650,  # Pressure_Differential\n",
    "            1  # Has_Annulus_Pressure\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            predictions = pipeline.predict_new_well(example_features)\n",
    "            print(\"\\nPredictions for example well:\")\n",
    "            for target, result in predictions.items():\n",
    "                print(f\"{target}: {result['prediction']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction: {e}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE EXECUTION COMPLETE!\")\n",
    "        print(\"Files generated:\")\n",
    "        print(\"- DataPhandas_DSEATS_Africa_2025_Classification.csv\")\n",
    "        print(\"- well_classification_analysis.png\")\n",
    "        print(\"- model_performance_analysis.png\")\n",
    "        print(\"- processed_features.csv\")\n",
    "        print(\"- target_data.csv\")\n",
    "        print(\"- gaussian_nb_model.pkl\")\n",
    "        print(\"- feature_scaler.pkl\")\n",
    "        print(\"- label_encoders.pkl\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
